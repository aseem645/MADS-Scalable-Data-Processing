{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIADS 516: Homework 1\n",
    "Version 1.0.20200221.1\n",
    "### Dr. Chris Teplovs, School of Information, University of Michigan\n",
    "<small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a>This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first mrjob script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the following example from the lectures:\n",
    "\n",
    "Note the use of the magic command ```%%file```.  You can use this to write the contents of a cell out to a file, which is what we need to do to use mrjob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_count.py\n"
     ]
    }
   ],
   "source": [
    "%%file word_count.py\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "  ### input: self, in_key, in_value\n",
    "  def mapper(self, _, line):\n",
    "    yield \"chars\", len(line)\n",
    "    yield \"words\", len(line.split())\n",
    "    yield \"lines\", 1\n",
    "\n",
    "  ### input: self, in_key from mapper, in_value from mapper\n",
    "  def reducer(self, key, values):\n",
    "    yield key, sum(values)\n",
    "if __name__ == \"__main__\":\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q1: Explain what each of the yield statements in the above script do.  Provide a list of what the first few iterations through the mapper() step would yield if the script was run against the ```data/gutenberg/short.t1.txt``` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first yield statement within the mapper function will, for each iteration through the text file, yield a tuple consisting of the key \"chars\", followed by the number of characters contained in the line. Whitespace and punctuation are counted as a character.\n",
    "2. The second yield statement within the mapper function will, for each iteration through the text file, yield a tuple consisting of the key \"words\", followed by the number of words contained in the line. This is obtained by utilizing the python String split() method, which will split a string into a list of elements(any given line in the txt file) and will, by default, seperate each element by whitespace. One can then take the len() of that list to obtain the number of words in the string. \n",
    "3. The third yield statement within the mapper function will, for each iteration through the text file, yield a tuple consisting of the key \"lines\", followed by the number of lines that was passed in. Since the mapper function consumes one line each iteration, this value can just be hardcoded as one.\n",
    "\n",
    "Line 1:Project Gutenberg's In the Year 2889, by Jules Verne and Michel Verne\n",
    "\n",
    "Line 2:\n",
    "\n",
    "Line 3: This eBook is for the use of anyone anywhere at no cost and with\n",
    "\n",
    "The output yielded for the first three lines for short.t1.txt would look like so:\n",
    "\n",
    "**Line 1**\n",
    "\"chars\" 69\n",
    "\"words\" 12\n",
    "\"lines\" 1\n",
    "\n",
    "**Line 2**\n",
    "\"chars\" 0\n",
    "\"words\" 0\n",
    "\"lines\" 1\n",
    "\n",
    "**Line 3**\n",
    "\"chars\" 64\n",
    "\"words\" 14\n",
    "\"lines\" 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the output of running the script against that same file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/word_count.jovyan.20200705.224821.096617\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/word_count.jovyan.20200705.224821.096617/output\n",
      "Streaming final output from /tmp/word_count.jovyan.20200705.224821.096617/output...\n",
      "\"chars\"\t10653\n",
      "\"words\"\t1822\n",
      "\"lines\"\t200\n",
      "Removing temp directory /tmp/word_count.jovyan.20200705.224821.096617...\n"
     ]
    }
   ],
   "source": [
    "!python word_count.py data/gutenberg/short.t1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q2.  Repeat the above cell using the the works of William Shakespeare text file (data/gutenberg/t8.shakespeare.txt).  Provide an interpretation of the output (don't overthink this -- just demonstrate that you can find the relevant information in the output).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/word_count.jovyan.20200705.224821.580163\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/word_count.jovyan.20200705.224821.580163/output\n",
      "Streaming final output from /tmp/word_count.jovyan.20200705.224821.580163/output...\n",
      "\"chars\"\t5333743\n",
      "\"words\"\t901325\n",
      "\"lines\"\t124456\n",
      "Removing temp directory /tmp/word_count.jovyan.20200705.224821.580163...\n"
     ]
    }
   ],
   "source": [
    "!python word_count.py data/gutenberg/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are not launching our map reduce job onto a specialized platform like Amazon EMR or Cloudera, we do not have to specify any configuration values(for instance, EMR may require a secret access key) - hence, the messages indicating \"No configs found.\" Upon running word_count.py, a temporary directory is created called tmp and creates an empty file called word_count.jovyan.20200703.202608.969845. Then, the intermediary mapper and final key, sum(values) are written to this file, and the output is printed to the console. For the final step, the temporary directory is destroyed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at a slightly more complicated example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting most_used_word.py\n"
     ]
    }
   ],
   "source": [
    "%%file most_used_word.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # any whitespace or apostrophe, used to split lines below\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "    STOPWORDS = {'i', 'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            if word.lower() not in self.STOPWORDS:\n",
    "                yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # optimization: sum the words we've seen so far\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is used so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    start = time.time()\n",
    "    MRMostUsedWord.run()\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q3: Explain what the yield statements in the  above script do.  Provide a list of what the first few iterations through the steps would yield."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mapper_get_words()** - The yield statement in this function yields a tuple of each word in the given line, converted to lowercase, and the count, hardcoded as one(potential duplicate words will be summed in the combiner function). Prior to retrieving a word in a given line, the function validates the word against the words in the STOPWORDS dictionary. If the word exists in the STOPWORDS dictionary, that word is not retrieved. \n",
    "\n",
    "**combiner_count_words()** - This function counts the number of time each word has been retrieved. It yields a tuple consisting of an individual word and that word's count. This function is essentially a mini reducer and its use is optional, but it can be useful to use for memory optimization purposes. \n",
    "\n",
    "**reducer_count_words()** - This function is the MapReduce reducer, which sums all occurrences of each word and yields a tuple of the form (counts, word). \n",
    "\n",
    "**reducer_find_max_word** - This function yields the max word/count pair after the reduce step in the form of a tuple, formatted as (counts, word) using the built in max() function.\n",
    "\n",
    "**Line 1**:North American plants are plants too\n",
    "\n",
    "(2, plants)\n",
    "\n",
    "**Line 2**: North American animals are American\n",
    "\n",
    "(3, American)\n",
    "\n",
    "**Line 3**: South American Ecosystems\n",
    "\n",
    "**(4, American)**\n",
    "\n",
    "Utilizing these three made up lines of text as a hypothetical example, the job will maintain a running count of the most utilized word. Upon consuming all of the lines, it would finally output the tuple (4, American), since the word American is the most utilized word in the text. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the file against data/gutenberg/short.t1.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/most_used_word.jovyan.20200705.224824.038916\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/most_used_word.jovyan.20200705.224824.038916/output\n",
      "Streaming final output from /tmp/most_used_word.jovyan.20200705.224824.038916/output...\n",
      "11\t\"day\"\n",
      "Removing temp directory /tmp/most_used_word.jovyan.20200705.224824.038916...\n",
      "0.7090287208557129\n"
     ]
    }
   ],
   "source": [
    "!python most_used_word.py data/gutenberg/short.t1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q4: Run the above script on the Shakespeare text file.  What answer do you get?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/most_used_word.jovyan.20200705.224824.963658\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/most_used_word.jovyan.20200705.224824.963658/output\n",
      "Streaming final output from /tmp/most_used_word.jovyan.20200705.224824.963658/output...\n",
      "5479\t\"thou\"\n",
      "Removing temp directory /tmp/most_used_word.jovyan.20200705.224824.963658...\n",
      "4.673604726791382\n"
     ]
    }
   ],
   "source": [
    "!python most_used_word.py data/gutenberg/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon running the most_used_word.py file against the Shakespeare text file, the most utilized word in the text is \"though\", with a count of 5479."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q5: What is the impact of removing the combiner from the above code in terms of efficiency?  What does that suggest?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting most_used_word.py\n"
     ]
    }
   ],
   "source": [
    "%%file most_used_word.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # any whitespace or apostrophe, used to split lines below\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "    STOPWORDS = {'i', 'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            if word.lower() not in self.STOPWORDS:\n",
    "                yield (word.lower(), 1)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is used so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    start = time.time()\n",
    "    MRMostUsedWord.run()\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/most_used_word.jovyan.20200705.224829.887801\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/most_used_word.jovyan.20200705.224829.887801/output\n",
      "Streaming final output from /tmp/most_used_word.jovyan.20200705.224829.887801/output...\n",
      "5479\t\"thou\"\n",
      "Removing temp directory /tmp/most_used_word.jovyan.20200705.224829.887801...\n",
      "3.8563339710235596\n"
     ]
    }
   ],
   "source": [
    "!python most_used_word.py data/gutenberg/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, by removing the combiner in the most_used_word.py, the amount of time taken to yield the final output is marginally faster (~7.6 vs. ~5.9). This might imply that adding another mini-reducer in the form of the combiner increases the amount of time taken for the job to complete, perhaps because it creates **additional** reduce tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q6: Write an mrjob script that finds the 10 words that have the most syllables from the t5.churchill.txt file.  Interpret your results.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting most_syllables_top_ten.py\n"
     ]
    }
   ],
   "source": [
    "%%file most_syllables_top_ten.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import syllapy\n",
    "\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # any whitespace or apostrophe, used to split lines below\n",
    "\n",
    "\n",
    "\n",
    "class MRMostSyllablesTopTen(MRJob):\n",
    "    \n",
    "    STOPWORDS = {'i', 'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}\n",
    "  \n",
    "   \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper_get_words), MRStep(reducer = self.reducer)]\n",
    "  \n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            if word.lower() not in self.STOPWORDS:\n",
    "                syllable_count =  syllapy.count(word)\n",
    "                yield None, (syllable_count, word.lower())\n",
    "        \n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        self.alist = []\n",
    "        for value in values:\n",
    "            self.alist.append(value)\n",
    "            self.blist = []\n",
    "        for i in range(10):\n",
    "            self.blist.append(max(self.alist))\n",
    "            self.alist.remove(max(self.alist))\n",
    "        for i in range(10):\n",
    "            yield self.blist[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    start = time.time()\n",
    "    MRMostSyllablesTopTen.run()\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/most_syllables_top_ten.jovyan.20200705.224834.233530\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/most_syllables_top_ten.jovyan.20200705.224834.233530/output\n",
      "Streaming final output from /tmp/most_syllables_top_ten.jovyan.20200705.224834.233530/output...\n",
      "8\t\"overcapitalization\"\n",
      "8\t\"incommunicability\"\n",
      "7\t\"unenforceability\"\n",
      "7\t\"overcapitalized\"\n",
      "7\t\"materialistically\"\n",
      "7\t\"invulnerability\"\n",
      "7\t\"interrogatively\"\n",
      "7\t\"infinitesimally\"\n",
      "7\t\"indissolubility\"\n",
      "7\t\"indispensability\"\n",
      "Removing temp directory /tmp/most_syllables_top_ten.jovyan.20200705.224834.233530...\n",
      "8.836472511291504\n"
     ]
    }
   ],
   "source": [
    "!python most_syllables_top_ten.py data/gutenberg/t5.churchill.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to divide this MapReduce job into two distinct steps(just one mapper and just one reducer) without any intermediary reducing(i.e. combiner). The mapper step retrieves all words in each line of the text, excluding the words specified in the STOPWORDS dictionary, and calculates the syllable count for each word using the external syllapy library. The mapper then yields each word, formatted as a tuple (syllable_count, word.lower()). The sorting happens within the reducer step. All syllable/word pairs are appended to alist. Then, the top ten max values within alist is appended to blist, using a for loop and the built in max() function. Then, all values within blist are appended to the list without_duplictes(duplicates removed using list comprehension). Then, the top ten elements in without_duplicates are yielded using a for loop. Based on the output, the words with the most syllables are 'overcapitalization' and 'incommunicability', at 8 syllables each. \n",
    "\n",
    "If the hardcoded range is substantially greater than 10, one starts to run into the issue of duplicates in the output, since, naturally, some words occur in the text more than once. While this is not an inherently bad thing, it results in visual clutter for larger requested outputs. When I attempted to remove duplicates by converting the final list of tuples to a set of tuples, through generic for loops and list comprehension, or through the numpy.unique() function, the operations ended up culling large swaths of entries from the final list, as opposed to only duplicate entries. For the purpose of this specific ask, however, this code does suffice. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
