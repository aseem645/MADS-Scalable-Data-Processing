{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xKUiifPp32x"
   },
   "source": [
    "# SIADS 516: Homework 3\n",
    "Version 1.0.20200303.2\n",
    "### Dr. Chris Teplovs, School of Information, University of Michigan\n",
    "<small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a>This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework assignment builds on the Spark DataFrame material we covered in class.\n",
    "You will be using a compressed version of the Yelp Academic Dataset.  The data set is provided for you in the data/yelp-academic sub-folder of this notebook's directory and you should not need to download it again if you're working on the Coursera hosted notebook environment.\n",
    "\n",
    "You might want to refer to the lecture companion notebooks (in workspace-files/resources/lecture_notebooks or equivalently via Coursera as \"Ungraded Lab: Spark Core Demo\" and \"Ungraded Lab: Spark SQL Demo) for hints about libraries to import, how to set up a SparkSession, and how to read data files.\n",
    "\n",
    "You will notice that there are a **lot** of reviews.  You might want to work off a small sample (i.e. use the sample() function in Spark) to work on a reduced size dataset while you're developing your solution.\n",
    "\n",
    "**You should take care to document your work, preferably using markdown blocks. In-code commenting is also \n",
    "a good idea.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q1: How many users have received more than 5000 cool compliments?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's set up a spark session to start off.\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('HW 3 Spark Application') \\\n",
    "    .getOrCreate() \n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#Now, let's import the yelp json files as a dataframe to begin answering Q1. We'll use the user df.\n",
    "\n",
    "# yelp_user_df = spark.read.json('../non_auto_assignments/data/yelp_academic/yelp_academic_dataset_user.json.gz')\n",
    "# yelp_business_df = ('../non_auto_assignments/data/yelp_academic/yelp_academic_dataset_business.json.gz')\n",
    "# yelp_tips_df = ('../non_auto_assignments/data/yelp_academic/yelp_academic_dataset_tip.json.gz')\n",
    "# yelp_checkin_df = spark.read.json('../non_auto_assignments/data/yelp_academic/yelp_academic_dataset_checkin.json.gz')\n",
    "# yelp_review_df = spark.read.json('../non_auto_assignments/data/yelp_academic/yelp_academic_dataset_review.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- average_stars: double (nullable = true)\n",
      " |-- compliment_cool: long (nullable = true)\n",
      " |-- compliment_cute: long (nullable = true)\n",
      " |-- compliment_funny: long (nullable = true)\n",
      " |-- compliment_hot: long (nullable = true)\n",
      " |-- compliment_list: long (nullable = true)\n",
      " |-- compliment_more: long (nullable = true)\n",
      " |-- compliment_note: long (nullable = true)\n",
      " |-- compliment_photos: long (nullable = true)\n",
      " |-- compliment_plain: long (nullable = true)\n",
      " |-- compliment_profile: long (nullable = true)\n",
      " |-- compliment_writer: long (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- elite: string (nullable = true)\n",
      " |-- fans: long (nullable = true)\n",
      " |-- friends: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- yelping_since: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's check out the schema as an FYI.\n",
    "yelp_user_df = spark.read.json('../non_auto_assignments/data/yelp_academic/yelp_academic_dataset_user.json.gz')\n",
    "yelp_user_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, let's retrieve the amount of users that have received > 5000 cool compliments by using the .filter function.\n",
    "cool_compliments = yelp_user_df.filter(yelp_user_df['compliment_cool'] > 5000).count()\n",
    "\n",
    "#Let's print the results to the console.\n",
    "cool_compliments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By utilizing the .filter function and boolean masking on the compliment_cool column, we are able to determine that the number of users that have received the compliment 'cool' over five-thousand times is seventy-nine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q2: What are the names of the top 10 most complimented businesses?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: We will ignore this question for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert your interpretation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q3: What are the top 10 most useful positive reviews?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For This question, we'll use the yelp_review_df again.\n",
    "\n",
    "yelp_review_df = spark.read.json('../non_auto_assignments/data/yelp_academic/yelp_academic_dataset_review.json.gz')\n",
    "\n",
    "yelp_review_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------+-----+\n",
      "|         business_id|             user_id|           review_id|useful|stars|\n",
      "+--------------------+--------------------+--------------------+------+-----+\n",
      "|t-o_Sraneime4DDhW...|aIbYxOV_3dBIPUcnl...|1lGXlyq4MALOMx17v...|   358|  5.0|\n",
      "|6Suj9mb9565xjAKHM...|d3U8ftbUpjuPQbacW...|gAUkgn4dTO-R2n5LB...|   278|  5.0|\n",
      "|IapQwLdAwztQYN99p...|UUqGHQFu2tQDGv5r3...|0nr6SQFKpR6JCYl1z...|   241|  5.0|\n",
      "|Ka00H3EHLLiPNpMj1...|--2vR0DIsmQ6WfcSz...|tTs6vjzf5Mvhs4aOA...|   215|  5.0|\n",
      "|igHYkXZMLAc9UdV5V...|7W-p-PJlmrzg0mk3p...|PAN7D4F6gzHELLMKs...|   215|  5.0|\n",
      "|3XsOOHcDC-XP8LPbp...|7W-p-PJlmrzg0mk3p...|vTMgyKHKHpWR0u8kN...|   210|  5.0|\n",
      "|PyKr1RX29U21B8NZq...|0o8HUzggoNKay9-ZM...|jZ7GeY_viZuYT2dkd...|   208|  5.0|\n",
      "|t-o_Sraneime4DDhW...|--2vR0DIsmQ6WfcSz...|CC0kHI2mVkdsQWVUx...|   207|  5.0|\n",
      "|7dHYudt6OOIjiaxkS...|--2vR0DIsmQ6WfcSz...|K8LGQQyUEPYjYuh6H...|   207|  5.0|\n",
      "|4ONpzAtnKbDig_e_O...|--2vR0DIsmQ6WfcSz...|rqeYJ-F26J87InZbK...|   203|  5.0|\n",
      "+--------------------+--------------------+--------------------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Since the meaning of positive is somewhat subjective, for the sake of concreteness, we'll consider 5-star reviews.\n",
    "\n",
    "yelp_review_df_positive = yelp_review_df.filter(yelp_review_df['stars'] == 5)\n",
    "\n",
    "#Now, we'll sort by the useful column in descending order, and also display the user_id, business id, stars, and review_id.\n",
    "\n",
    "yelp_review_df_positive = yelp_review_df_positive.sort('useful', ascending = False)\n",
    "\n",
    "yelp_review_df_positive.select(['business_id', 'user_id', 'review_id', 'useful', 'stars']).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When setting a minimum threshold for positive reviews at five stars, the most useful review was noted as useful a total of three hundred and fifty-eight times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q4: During what hour of the day do most checkins occur?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In order to begin answering this question, we'll import the checkin.json file as a df. \n",
    "yelp_checkin_df = spark.read.json('../non_auto_assignments/data/yelp_academic/yelp_academic_dataset_checkin.json.gz')\n",
    "\n",
    "\n",
    "#As usual, let's print the schema as an FYI.\n",
    "yelp_checkin_df.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                date|\n",
      "+--------------------+\n",
      "|2016-04-26 19:49:...|\n",
      "|2011-06-04 18:22:...|\n",
      "|2014-12-29 19:25:...|\n",
      "| 2016-07-08 16:43:30|\n",
      "|2010-06-26 17:39:...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's check out the date column, so we get a better understanding of how it's formatted.\n",
    "yelp_checkin_df.select('date').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|just_hour_only|count|\n",
      "+--------------+-----+\n",
      "|            19|13481|\n",
      "|            23|13207|\n",
      "|            22|13191|\n",
      "|            18|13177|\n",
      "|            21|12960|\n",
      "|            20|12553|\n",
      "|            17|12304|\n",
      "|             0|11577|\n",
      "|            16|10416|\n",
      "|             1| 9803|\n",
      "+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Lets first split the 'date' column into two seperate ones, one with just the date and one with just hour:minutes:seconds.\n",
    "yelp_checkin_df_split = yelp_checkin_df.withColumn(\"just_date\", F.split(F.col(\"date\"), \" \").getItem(0)).withColumn(\"just_time\", F.split(F.col(\"date\"), \" \").getItem(1))\n",
    "\n",
    "#Now let's remove the extraneous comma from the just time field.\n",
    "yelp_checkin_df_split = yelp_checkin_df_split.select(\"just_time\", F.regexp_replace(F.col(\"just_time\"), \"[\\$#,]\", \"\").alias(\"just_time_no_comma\"))\n",
    "\n",
    "#Now we can try to get the hour of the day from this, using the 24 hour clock.\n",
    "yelp_checkin_df_split = yelp_checkin_df_split.withColumn('just_hour_only',F.hour('just_time_no_comma'))\n",
    "\n",
    "#Now, utilizing the just_hour_only column, we can count the occurrences of each hour, and sort in descending order.\n",
    "yelp_checkin_df_split.groupBy('just_hour_only').count().sort('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output, the most common time that customers are checking in to businesses is 19:00, or 7:00 PM, followed by 11:00 PM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q5: Sentiment analysis</font>\n",
    "\n",
    "a. List the 50 most common non-stopword words that are unique to *positive* reviews.\n",
    "b. List the 50 most common non-stopword words that are unique to *negative* reviews.\n",
    "\n",
    "You can use the stopword list that was introduced in the lecture materials or you can \n",
    "find/devise one of your own.\n",
    "\n",
    "You will need to define what constitutes a positive review and what constitutes a negative review.  We highly recommend that you use the number of stars to figure this out.  Be sure to provide a rationale for your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider the following two reviews:\n",
    "\n",
    "* Positive: The meal was great, and the service was the best we ever experienced.\n",
    "* Negative: The meal was awful.  It was the worst thing we ever experienced.\n",
    "\n",
    "Assume our stopwords are {'the','was','and','the','was','we','it'}\n",
    "\n",
    "* Positive unique: {'great', 'service', 'best'}\n",
    "\n",
    "* Negative unique: {'awful', 'worst', 'thing'}\n",
    "\n",
    "In this example, each unique word occurs just once, so the concept of \"top 50\" doesn't make sense.  For your data, you'll need to count the number of times each unique word occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we've already defined a positive review dataframe, let's create a negative review dataframe.\n",
    "#The negative review dataframe will consist of reviews that were allocated only one star.\n",
    "yelp_review_df_negative = yelp_review_df.filter(yelp_review_df['stars'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to begin answering this question, we'll import the re library.\n",
    "import re\n",
    "\n",
    "\n",
    "#Next we'll define the stopwords list\n",
    "STOPWORDS = ['i',  '-', '&', \"I've\", \"I'm\", '2', 'it.' \"it's\",  'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "STOPWORDS_UPPER = [word.title() for word in STOPWORDS]\n",
    "\n",
    "#Let's create a count dataframe for positive\n",
    "count_df_positive = yelp_review_df_positive.withColumn('word', F.explode(F.split(F.col('text'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\n",
    "\n",
    "\n",
    "#Let's create a count dataframe for negative\n",
    "count_df_negative = yelp_review_df_negative.withColumn('word', F.explode(F.split(F.col('text'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Now let's filter out the stopwords from the dataframe, while also filtering out any potential uppercase duplicates.\n",
    "count_df_positive = count_df_positive.filter(~F.col('word').isin(STOPWORDS))\n",
    "count_df_positive = count_df_positive.filter(~F.col('word').isin(STOPWORDS_UPPER))\n",
    "\n",
    "count_df_negative = count_df_negative.filter(~F.col('word').isin(STOPWORDS))\n",
    "count_df_negative = count_df_negative.filter(~F.col('word').isin(STOPWORDS_UPPER))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|      word|  count|\n",
      "+----------+-------+\n",
      "|          |4033319|\n",
      "|     great| 986721|\n",
      "|     place| 978963|\n",
      "|      food| 753989|\n",
      "|       get| 668976|\n",
      "|      good| 645730|\n",
      "|      like| 644250|\n",
      "|      time| 635453|\n",
      "|       one| 557158|\n",
      "|    really| 533162|\n",
      "|        go| 531548|\n",
      "|    always| 531538|\n",
      "|   service| 529984|\n",
      "|      best| 518238|\n",
      "|     would| 515217|\n",
      "|      back| 509980|\n",
      "|      also| 480754|\n",
      "|      love| 445954|\n",
      "| recommend| 400349|\n",
      "|definitely| 390759|\n",
      "|       got| 388276|\n",
      "|     staff| 374825|\n",
      "|        us| 366962|\n",
      "|      even| 365423|\n",
      "|      it's| 360287|\n",
      "|      made| 338603|\n",
      "|  friendly| 334469|\n",
      "|      nice| 324736|\n",
      "|      come| 316480|\n",
      "|     Great| 314480|\n",
      "|      make| 308374|\n",
      "|     first| 303408|\n",
      "|       try| 292827|\n",
      "|    little| 285873|\n",
      "|      came| 284645|\n",
      "|       new| 282948|\n",
      "|     don't| 276007|\n",
      "|     never| 274331|\n",
      "|   amazing| 271350|\n",
      "|     going| 265211|\n",
      "|      went| 263916|\n",
      "|     every| 260266|\n",
      "|     could| 258111|\n",
      "|      much| 252507|\n",
      "|      ever| 251153|\n",
      "|      well| 248620|\n",
      "|       it.| 244600|\n",
      "|   ordered| 235901|\n",
      "|      know| 232776|\n",
      "|      It's| 229003|\n",
      "|      sure| 225335|\n",
      "+----------+-------+\n",
      "only showing top 51 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Outputting top 51 since the first row just displays a count of null values. We want 50 actual words\n",
    "count_df_positive.show(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|    word|  count|\n",
      "+--------+-------+\n",
      "|        |2069997|\n",
      "|   would| 500736|\n",
      "|     get| 484152|\n",
      "|    like| 385264|\n",
      "|     one| 381989|\n",
      "|    told| 353544|\n",
      "|    back| 350734|\n",
      "|    said| 342816|\n",
      "|   place| 339902|\n",
      "|    time| 339723|\n",
      "|    even| 332486|\n",
      "|    food| 325432|\n",
      "|      us| 302851|\n",
      "|      go| 301994|\n",
      "|   never| 297743|\n",
      "| service| 296364|\n",
      "|  didn't| 272059|\n",
      "|     got| 267761|\n",
      "|   don't| 257549|\n",
      "|   asked| 253132|\n",
      "|   could| 250926|\n",
      "|    went| 215631|\n",
      "|    came| 211249|\n",
      "|   going| 192374|\n",
      "|   order| 190095|\n",
      "| minutes| 184698|\n",
      "|  called| 183210|\n",
      "|  people| 182957|\n",
      "|    good| 178774|\n",
      "|customer| 178563|\n",
      "|    know| 178459|\n",
      "| another| 173658|\n",
      "|    give| 167998|\n",
      "|    take| 167166|\n",
      "|    come| 165141|\n",
      "|    took| 163506|\n",
      "| ordered| 157989|\n",
      "|    make| 157014|\n",
      "|   still| 153048|\n",
      "|    call| 149798|\n",
      "|    it's| 148363|\n",
      "|     it.| 147790|\n",
      "|  really| 146852|\n",
      "|   first| 146311|\n",
      "|    want| 145889|\n",
      "|     car| 143276|\n",
      "|     two| 139820|\n",
      "|    ever| 137625|\n",
      "| manager| 131854|\n",
      "|     see| 125787|\n",
      "|    made| 125652|\n",
      "+--------+-------+\n",
      "only showing top 51 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Outputting top 51 rows since the first row just displays a count of null values. We want 50 actual words\n",
    "count_df_negative.show(51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this analysis, in order to capture the more salient differences between negative and positive reviews, I manually set the minimum standard for 'positive reviews' at an allocation of 5 stars, while I considered all 1 star reviews 'negative'. I chose values on the opposite end of the customer experience spectrum based on my own intuition, since values in between these values(1.5 - 4.5) could be considered as mixed bags, in the sense that reviewers may still compose a largely positive or largely negative review at one of these intermediate values, but still include a few minor points that detracted or added to their experience, resulting in a review of more mixed sentiment, thus giving rise to the increased potentiality of duplciate describers. \n",
    "\n",
    "\n",
    "\n",
    "Upon viewsing the output of count_df_positive, several clear positive descriptors stand out. Words like \"amazing\", \"great\", \"good\", \"friendly\", and, \"recommend\", and  \"nice\" are utilized, with 'great' holding the number one spot. The words \"staff\" and \"service\" are also called out, likely used in conjunction with the adjectivses above. The words \"would\", \"go\", and \"back\" are also identified as highly utilized, and one can infer that they make up the string \"would go back.\" If a customer has an emphatically positive experience, they would naturally frequent it, and urge other customers to do the same. \n",
    "\n",
    "\n",
    "The words listed in the output from the count_df_negative dataframe is much less abrasive than I thought, having anticipated to see adjectives with highly negative connotations, like \"awful\" or \"terrible.\" In any case, it is possible to infer that the output comes from the negative dataframe. The contractions 'don't' and 'didn't' appear in the list, along with the words \"never\" and \"got,\" all suggesting an experience that was lacking in some fashion. The word \"manager\" also appears in the output, whereas it does not appear in the output from the positive dataframe. This may suggest that customer experiences involving the manager of an establishment are, by and large, negative. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
